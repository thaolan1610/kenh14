import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from urllib.parse import urljoin
import schedule
import datetime

BASE_URL = "https://kenh14.vn"
CATEGORY_PATH = "/xa-hoi.chn"

# L·∫•y link t·ª´ng b√†i vi·∫øt tr√™n 1 trang
def get_articles_from_page(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    articles = []
    for item in soup.find_all("div", class_="knswli-right"):
        title_tag = item.find("h3")
        if title_tag and title_tag.a:
            title = title_tag.get_text(strip=True)
            link = urljoin(BASE_URL, title_tag.a["href"])
            articles.append({"title": title, "url": link})
    return articles

# L·∫•y th√¥ng tin chi ti·∫øt t·ª´ng b√†i vi·∫øt
def get_article_details(article):
    url = article.get("url")
    if not url:
        print("‚ùå B·ªè qua v√¨ thi·∫øu URL:", article)
        return {}

    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    # M√¥ t·∫£ t·ª´ <meta name="description">
    meta_desc = soup.find("meta", attrs={"name": "description"})
    description = meta_desc["content"].strip() if meta_desc and meta_desc.get("content") else ""

    # ·∫¢nh b√†i vi·∫øt
    image_url = ""
    image_meta = soup.find("meta", property="og:image")
    if image_meta:
        image_url = image_meta.get("content", "")

    # N·ªôi dung b√†i vi·∫øt
    content = ""
    content_div = soup.find("div", class_="knc-content")
    if content_div:
        paragraphs = content_div.find_all("p")
        content = "\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

    return {
        "Ti√™u ƒë·ªÅ": article["title"],
        "M√¥ t·∫£": description,
        "H√¨nh ·∫£nh": image_url,
        "N·ªôi dung": content,
        "URL": article["url"]
    }

# Thu th·∫≠p v√† l∆∞u d·ªØ li·ªáu t·ª´ nhi·ªÅu trang
def collect_data():
    full_data = []
    max_pages = 5  

    for page in range(1, max_pages + 1):
        url = f"https://kenh14.vn/xa-hoi/trang-{page}.chn"
        print(f"üìÑ ƒêang l·∫•y d·ªØ li·ªáu trang {page}: {url}")
        try:
            articles = get_articles_from_page(url)
            if not articles:
                print("‚úÖ Kh√¥ng c√≤n b√†i vi·∫øt n√†o, d·ª´ng l·∫°i.")
                break
            for article in articles:
                detail = get_article_details(article)
                if detail:
                    full_data.append(detail)
            time.sleep(1)  # ‚è≥ ngh·ªâ gi·ªØa c√°c trang
        except Exception as e:
            print(f"‚ùå L·ªói ·ªü trang {page}: {e}")

    if full_data:
        df = pd.DataFrame(full_data)
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        excel_filename = f"kenh14_xahoi_{timestamp}.xlsx"
        df.to_excel(excel_filename, index=False)
        print(f"‚úÖ ƒê√£ l∆∞u {len(full_data)} b√†i vi·∫øt v√†o {excel_filename}")
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ b√†i vi·∫øt n√†o.")

# L·∫≠p l·ªãch ch·∫°y l√∫c 6h s√°ng m·ªói ng√†y
def job():
    print(f"üïï [{datetime.datetime.now()}] B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu Kenh14...")
    collect_data()

if __name__ == "__main__":
   
    schedule.every().day.at("06:00").do(job)  # L·ªãch ch·∫°y l√∫c 6h s√°ng
    job()
    while True:
        schedule.run_pending()  # Ki·ªÉm tra l·ªãch tr√¨nh
        time.sleep(60)  # Ch·ªù 1 ph√∫t r·ªìi ki·ªÉm tra l·∫°i
