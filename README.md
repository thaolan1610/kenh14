# Kenh14 Scraper

CÃ´ng cá»¥ thu tháº­p dá»¯ liá»‡u tá»« chuyÃªn má»¥c **XÃ£ há»™i** cá»§a trang [Kenh14.vn](https://kenh14.vn/xa-hoi.chn) vÃ  lÆ°u vÃ o file CSV.

##TÃ­nh nÄƒng
Thu tháº­p tiÃªu Ä‘á», mÃ´ táº£, ná»™i dung vÃ  áº£nh Ä‘áº¡i diá»‡n tá»« cÃ¡c bÃ i viáº¿t.

LÆ°u dá»¯ liá»‡u vÃ o file Excel .

Tá»± Ä‘á»™ng cháº¡y hÃ ng ngÃ y lÃºc 6:00 sÃ¡ng.


## YÃªu cáº§u há»‡ thá»‘ng

- Python 3.8 trá»Ÿ lÃªn
- pip (trÃ¬nh quáº£n lÃ½ gÃ³i Python)

## CÃ i Ä‘áº·t

1. **Clone project vá» mÃ¡y:**

git clone https://github.com/your-username/kenh14-scraper.git
cd kenh14-scraper
2.CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:
pip install -r requirements.txt
3. Náº¿u muá»‘n xuáº¥t ra file .xlsx, cáº§n cÃ i thÃªm:
pip install openpyxl

## File requirements.txt
requests

beautifulsoup4

pandas

schedule

openpyxl
## kÃªnh14_scraper.py
1. Import thÆ° viá»‡n:
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from urllib.parse import urljoin
import schedule
import datetime
2. Khai bÃ¡o Ä‘Æ°á»ng dáº«n gá»‘c
BASE_URL = "https://kenh14.vn"
CATEGORY_PATH = "/xa-hoi.chn"
3. Láº¥y link bÃ i viáº¿t trÃªn má»™t trang
   def get_articles_from_page(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")

    articles = []
    for item in soup.find_all("div", class_="knswli-right"):
        title_tag = item.find("h3")
        if title_tag and title_tag.a:
            title = title_tag.get_text(strip=True)
            link = urljoin(BASE_URL, title_tag.a["href"])
            articles.append({"title": title, "url": link})
    return articles
4. Láº¥y ná»™i dung chi tiáº¿t tá»«ng bÃ i viáº¿t
def get_article_details(article):
    url = article.get("url")
    if not url:
        print("âŒ Bá» qua vÃ¬ thiáº¿u URL:", article)
        return {}

    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
-Kiá»ƒm tra URL bÃ i viáº¿t.
+Láº¥y ná»™i dung HTML cá»§a bÃ i viáº¿t.
    meta_desc = soup.find("meta", attrs={"name": "description"})
    description = meta_desc["content"].strip() if meta_desc and meta_desc.get("content") else ""
+Láº¥y pháº§n mÃ´ táº£ tá»« tháº» <meta name="description">.
    image_url = ""
    image_meta = soup.find("meta", property="og:image")
    if image_meta:
        image_url = image_meta.get("content", "")
+láº¥y áº£nh Ä‘áº¡i diá»‡n bÃ i viáº¿t tá»« tháº» <meta property="og:image">
    content = ""
    content_div = soup.find("div", class_="knc-content")
    if content_div:
        paragraphs = content_div.find_all("p")
        content = "\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
+Láº¥y ná»™i dung vÄƒn báº£n trong bÃ i viáº¿t, tá»« cÃ¡c tháº» <p> bÃªn trong <div class="knc-content">.
    return {
        "TiÃªu Ä‘á»": article["title"],
        "MÃ´ táº£": description,
        "HÃ¬nh áº£nh": image_url,
        "Ná»™i dung": content,
        "URL": article["url"]
    }
+Tráº£ vá» má»™t dict chá»©a toÃ n bá»™ thÃ´ng tin bÃ i viáº¿t.
5. Thu tháº­p nhiá»u trang vÃ  lÆ°u file Excel
  def collect_data():
    full_data = []
    max_pages = 5
+Thu tháº­p tá»« 5 trang Ä‘áº§u tiÃªn cá»§a chuyÃªn má»¥c XÃ£ há»™i.
    for page in range(1, max_pages + 1):
        url = f"https://kenh14.vn/xa-hoi/trang-{page}.chn"
        print(f"ğŸ“„ Äang láº¥y dá»¯ liá»‡u trang {page}: {url}")
        try:
            articles = get_articles_from_page(url)
            if not articles:
                print("âœ… KhÃ´ng cÃ²n bÃ i viáº¿t nÃ o, dá»«ng láº¡i.")
                break
            for article in articles:
                detail = get_article_details(article)
                if detail:
                    full_data.append(detail)
            time.sleep(1)
        except Exception as e:
            print(f"âŒ Lá»—i á»Ÿ trang {page}: {e}")
+Láº·p qua tá»«ng trang.
+Láº¥y danh sÃ¡ch bÃ i viáº¿t vÃ  ná»™i dung chi tiáº¿t tá»«ng bÃ i.
+Láº¥y danh sÃ¡ch bÃ i viáº¿t vÃ  ná»™i dung chi tiáº¿t tá»«ng bÃ i.
    if full_data:
        df = pd.DataFrame(full_data)
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        excel_filename = f"kenh14_xahoi_{timestamp}.xlsx"
        df.to_excel(excel_filename, index=False)
        print(f"âœ… ÄÃ£ lÆ°u {len(full_data)} bÃ i viáº¿t vÃ o {excel_filename}")
    else:
        print("âš ï¸ KhÃ´ng cÃ³ bÃ i viáº¿t nÃ o.")
+Táº¡o file .xlsx chá»©a toÃ n bá»™ dá»¯ liá»‡u.
6. Thiáº¿t láº­p cÃ´ng viá»‡c theo lá»‹ch
def job():
    print(f"ğŸ•• [{datetime.datetime.now()}] Báº¯t Ä‘áº§u thu tháº­p dá»¯ liá»‡u Kenh14...")
    collect_data()
7. Cháº¡y chÃ­nh: tá»± Ä‘á»™ng má»—i ngÃ y
if __name__ == "__main__":
    schedule.every().day.at("22:17").do(job)  # Hoáº·c "06:00"
    job()  # Cháº¡y ngay khi khá»Ÿi Ä‘á»™ng
    while True:
        schedule.run_pending()
        time.sleep(60)
+Gá»i job()láº§n Ä‘áº§u tiÃªn.
+Sau Ä‘Ã³ cá»© 1 phÃºt kiá»ƒm tra náº¿u tá»›i lá»‹ch thÃ¬ cháº¡y láº¡i.
## Tá»± Ä‘á»™ng thu tháº­p má»—i ngÃ y
Sau khi cháº¡y script, chÆ°Æ¡ng trÃ¬nh sáº½ tá»± Ä‘á»™ng:

Thu tháº­p bÃ i viáº¿t tá»« 5 trang Ä‘áº§u cá»§a chuyÃªn má»¥c XÃ£ há»™i.

Táº¡o file Excel chá»©a toÃ n bá»™ dá»¯ liá»‡u.

Láº­p lá»‹ch cháº¡y hÃ ng ngÃ y lÃºc 06:00 sÃ¡ng (cÃ³ thá»ƒ thay Ä‘á»•i theo nhu cáº§u).
## CÃ¡ch cháº¡y chÆ°Æ¡ng trÃ¬nh
python kenh14_scraper.py




